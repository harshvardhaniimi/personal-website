---
title: Score and Fisher's Information
subtitle: Intuitive Understanding and Mathematical Notions
summary: Intuitive Understanding and Mathematical Notions
author: Harshvardhan
date: '2022-02-13'
slug: score-and-fisher-s-information
categories:
  - statistics
tags: []
header-includes:
- \usepackage{amsmath}
bibliography: references.bib
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>Likelihood is a measure of how probable a parameter is, given our data. It is calculated as the joint probability of the observed data as a function of parameters for a given statistical model. Since joint probabilities are usually products, it is much easier to deal with log-likelihood which is a sum of probabilities.</p>
<p>Note that, likelihood is not the PDF of parameters. In maximum likelihood estimation, the likelihood function (i.e. the joint probability) is maximised to obtain a specific value of parameter. This value is “most likely” to be the true but unknown parameter. Generally, true parameter is denoted as <span class="math inline">\(\theta\)</span> and it’s estimate is denoted as <span class="math inline">\(\hat{\theta}\)</span>.</p>
</div>
<div id="score" class="section level2">
<h2>Score</h2>
<p>Score is a derivative of log-likelihood function with respect to the parameter. It is usually evaluated at a particular value of the parameter.</p>
<p><span class="math display">\[
s(\theta) = \frac{\partial \log \mathcal{L}(\theta)}{\partial \theta}
\]</span></p>
<p>This differentiation will lead to a <span class="math inline">\(1 \times m\)</span> vector. This vector indicates the sensitivity of the likelihood. Under certain regularity conditions<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, we can prove that the expected value of <span class="math inline">\(s(\theta)\)</span> is zero when evaluated at the true <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[
E \left[ \frac{\partial}{\partial \theta} \log f(X; \theta) \bigg\rvert \theta \right] \\
= \int_\mathbb{R} \frac{\frac{\partial}{\partial \theta} f(x; \theta)}{f(x;\theta)}
f(x;\theta) dx \\
= \frac{\partial}{\partial \theta} \int_{\mathbb{R}} f(x;\theta) dx \\
= \frac{\partial}{\partial \theta} 1 \\
= 0.
\]</span></p>
</div>
<div id="fishers-information" class="section level2">
<h2>Fisher’s Information</h2>
<p>Fisher’s information is a way to measure how much information is contained in a known random variable <span class="math inline">\(X\)</span> about the unknown population parameter <span class="math inline">\(\theta\)</span> that is supposed to model <span class="math inline">\(X\)</span>. It is calculated as the variance of the score.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>Imagine you want to estimate how good an estimate is given all our knowledge about it. Fisher’s information describes the probability <span class="math inline">\(f(x)\)</span>, given a known value of <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(f(x)\)</span> is sharply peaked at a value of <span class="math inline">\(X\)</span>, it indicates we have a good estimate of the true <span class="math inline">\(X\)</span>. If <span class="math inline">\(f(x)\)</span> is more evenly spread, we know little about the true value. Consequently we would need many more samples to accurately know what the value should be. Theoretically, we would need to know the entire population.</p>
<p>This intuitive understanding tells us that there would be some way to know how much information we have or how much information we need. Thus, there has to be a measure of variance with respect to <span class="math inline">\(\theta\)</span>.</p>
<p>Therefore, Fisher’s information is defined as the variance of the score, <span class="math inline">\(s(\theta)\)</span>.</p>
<p><span class="math display">\[
I(\theta) = E \left[ \left(\frac{\partial}{\partial \theta} \log f(X; \theta \right)^2 \bigg\rvert \theta\right].
\]</span></p>
<p>For a continuous PDF, this can be evaluated as the following.</p>
<p><span class="math display">\[
I(\theta) = \int_\mathbb{R} \left( \frac{\partial}{\partial \theta} \log f(x;\theta) \right)^2 f(x;\theta) dx.
\]</span></p>
<p>Since this is variance and has a square term, <span class="math inline">\(I(\theta)\)</span> is always non-negative.</p>
<p>Sometimes, it is also denoted as the following.</p>
<p><span class="math display">\[
I(\theta) = -E \left[ \frac{\partial^2}{\partial \theta^2} \log f(X; \theta) \bigg\rvert \theta \right],
\]</span></p>
<p>as</p>
<p><span class="math display">\[
\frac{\partial^2}{\partial \theta^2} \log f(X; \theta) = \frac{\frac{\partial^2}{\partial \theta^2} f(X; \theta)}{f(X; \theta)} - \left( \frac{\frac{\partial}{\partial \theta} f(X; \theta)}{f(X; \theta)} \right)^2 \\
= \frac{\frac{\partial^2}{\partial \theta^2} f(X; \theta)}{f(X; \theta)} - \left( \frac{\partial}{\partial \theta} \log f(X; \theta) \right)^2.
\]</span></p>
<p>Also note that</p>
<p><span class="math display">\[
E \left[ \frac{\frac{\partial^2}{\partial \theta^2} f(X; \theta)}{f(X; \theta)} \bigg\rvert \theta \right] = \frac{\partial^2}{\partial \theta^2} \int_\mathbb{R} f(x; \theta) dx = 0,
\]</span></p>
<p>as proved earlier in Score. Putting this result in the last equation, we obtain the expected result.</p>
</div>
<div id="cramer-rao-bound" class="section level2">
<h2>Cramer-Rao Bound</h2>
<p>Cramer-Rao bound is a related concept. It states that the inverse of Fisher information is a lower bound on the variance of any unbiased estimator <span class="math inline">\(\theta\)</span>. That is,</p>
<p><span class="math display">\[
V (\hat{\theta}) \geq \frac{1}{I(\theta)}.
\]</span></p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>To estimate the population parameter using maximum likelihood approach, we need to assume certain conditions about our probability density and likelihood functions. Continuity assumption is easily satisfied in real life but compactness isn’t as parameter space is often unbounded. Even when it is bounded, the bounds are usually unknown. For more details, see <a href="https://en.wikipedia.org/wiki/Likelihood_function#Regularity_conditions">this</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>All that follows is a correct but strong simplification of the complex concepts. For more mathematical notions, check <a href="https://en.wikipedia.org/wiki/Fisher_information">Wikipedia</a> and <a href="https://arxiv.org/pdf/1705.01064.pdf">Ly et al. (2017)</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
