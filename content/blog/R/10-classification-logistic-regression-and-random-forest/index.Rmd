---
title: 'Classification: Logistic Regression and Random Forest'
subtitle: "Using Tidymodels to Find Sex of Penguins"
summary: "Using Tidymodels to Find Sex of Penguins"
author: Harshvardhan
date: '2022-02-11'
slug: classification-logistic-regression-and-random-forest
categories:
  - R
tags: []
---

In this tutorial-cum-note, I will demonstrate how to use Logistic Regression and Random Forest algorithms to predict sex of a penguin. The data `penguins` comes from `palmerpenguins` package in R. It was collected by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) on three species of penguins at the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), a member of theÂ [Long Term Ecological Research Network](https://lternet.edu/).

**The goal is to build a classifier model that predicts sex of the penguins given its physical characteristics.**

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(dev = 'svg') # set output device to svg

theme_h = function(base_size = 14) {
  theme_bw(base_size = base_size) %+replace%
    theme(
      # Specify plot title
      plot.title = element_text(size = rel(1), face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Specifying grid and border
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Specidy axis details
      axis.title = element_text(size = rel(0.85), face = "bold"),
      axis.text = element_text(size = rel(0.70), face = "bold"),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # Specify legend details
      legend.title = element_text(size = rel(0.85), face = "bold"),
      legend.text = element_text(size = rel(0.70), face = "bold"),
      legend.key = element_rect(fill = "transparent", colour = NA),
      legend.key.size = unit(1.5, "lines"),
      legend.background = element_rect(fill = "transparent", colour = NA),
      # Remove default background
      strip.background = element_rect(fill = "#17252D", color = "#17252D"),
      strip.text = element_text(size = rel(0.85), face = "bold", color = "white", margin = margin(5,0,5,0))
    )
}
```

The dataset can be installed from CRAN.

```{r}
# If you don't have palmerpenguins package, first install it.
# install.packages("palmerpenguins")

# Loading Libraries
library(tidyverse)
library(palmerpenguins)

# setting my personal theme
theme_set(theme_h())

# Loading data
data("penguins")
penguins
```

We see that there are many missing instances. Let's see how many of them are missing.

```{r}
sum(is.na(penguins))
```

So, nineteen entries are missing. Most likely, I will exclude them from the analysis at present but before that, I want to explore the data as it is.

# Exploration

One of the best methods to do it is via `count()` from dplyr.

```{r}
penguins %>% 
   count(species)

penguins %>% 
   count(island)

penguins %>% 
   count(species, island)

penguins %>% 
   count(sex)

penguins %>% 
   count(year)
```

Cool. It looks pretty balanced.

```{r}
penguins %>% 
   filter(!is.na(sex)) %>% 
   ggplot(aes(flipper_length_mm, bill_length_mm, colour = sex, size = body_mass_g)) +
   geom_point(alpha = 0.6) +
   facet_wrap(~species)
```

In general, there is significant difference between `bill_length_mm` for both the sexes. Also, bill length for Adelie is shorter than other two species --- for moth the sexes.

There are also packages like [DataExplorer](https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html) that can aid in the process. For this simplistic case, I want to demonstrate how to create classification model, so I will ignore the process.

The dataset has missing values as I noted earlier. So, I will remove the observations with missing values. I also do not need year and island for my classification model --- there is no logical reason why should they be affecting sex of a penguin.

```{r}
penguins_df = penguins %>% 
   filter(!is.na(sex)) %>% 
   select(-year, -island)
```

# Modelling

Let's start by loading `tidymodels` and setting the seed for randomisation.

```{r}
library(tidymodels)
set.seed(1)
```

The first step of modelling is to create the training and testing split. Validation set will never be exposed to us during the modelling process. Once our final model is made, we can test it against the validation set.

In Tidy Models, this split is created using `initial_split` function. I also have the option to stratify the split --- which is necessary because we have multiple species and they are unbalanced. We only have 68 Chinstrap penguins but have 152 Adelie and 124 Gentoo. Let's explore the help file for `initial_split` too.

```{r}
?initial_split

# Specify the split
penguins_split = initial_split(penguins_df, strata = sex)
```

The proportion is set by default to 75% in training and 25% in testing. The functions `training()` and `testing()` will give me the resulting datasets.

```{r}
penguins_train = training(penguins_split)
penguins_test = testing(penguins_split)

penguins_train
penguins_test
```

The datasets looks good; our split worked well.

## Bootstrapping Samples

Note that our training sample has only 249 rows. This is not a huge dataset and we are not sure if the model we will make from this will be generalisable.

One simple way to solve this is using Bootstrapped samples. Each bootstrap sample is of original sample size but is also different from each other. They are collected sample of observations with replacement. Again, see the help file to understand the function.

```{r}
?bootstraps

penguins_boot = bootstraps(penguins_train)
penguins_boot
```

So, we have 25 bootstrapped samples each with different resamples. Of course, if you have enough data or are confident enough about your data, you can skip this step.

## Logistic Regression Pipeline

Logistic regression model is one of the simplest classification model. It is also the basic building block of neural networks; it dictates how a node behaves. Until 2010 when neural networks and support vector machines gained popularity, logistic regression was the model in force.

Even today, the model is widely used in variety of real world applications. The biggest benefit of logistic regression models is its ability to explain and linear implementation.

The first step will be to set up model pipeline. This only sets up how the model would work and neither training or test has happened yet.

```{r}
# Simple Logistic Regression
glm_spec = logistic_reg() %>% 
   set_engine("glm")

glm_spec
```

There are other alternatives too. We can use Lasso regression (yes, Lasso [can be used](https://stats.stackexchange.com/a/340963/185332) for classification as well. It "estimate[s] the parameters of the binomial GLM by optimising the binomial likelihood whilst imposing the lasso penalty on the parameter estimates".) Or we can just use a regularised classification model.

```{r}
# regularised regression
glm_spec = logistic_reg() %>% 
   set_engine("glmnet")

# LASSO regression
glm_spec = logistic_reg(mixture = 1) %>% 
   set_engine("glmnet")
```

But for this simple tutorial, I will stick to simple logistic regression model.

```{r}
# Simple Logistic Regression
glm_spec = logistic_reg() %>% 
   set_engine("glm")
```

## Random Forest Pipeline

Let's set up a pipeline for random forest model as well. The good part about random forest model is that they do not require huge tuning efforts like neural networks.

Random forest models can be used for classification as well as regression. Furthermore, there are many implementations (packages) in R to choose from. `randomForest` is probably the most known one. `ranger` is a fast implementation of random forest models in R. I will use `ranger` for this model.

```{r}
# Engine could be spark
rand_forest() %>% 
   set_mode("classification") %>% 
   set_engine("spark")

# Or it could be randomForest
rand_forest() %>% 
   set_mode("classification") %>% 
   set_engine("randomForest")

# Or ranger
rf_spec = rand_forest() %>% 
   set_mode("classification") %>% 
   set_engine("ranger")
rf_spec
```

## Workflow

The next step in modelling pipeline is setting up the model with formula, model and data --- in that order. Because I have multiple models that I want to compare, I will only set up formula in my workflow.

```{r}
penguin_wf = workflow() %>% 
   add_formula(sex ~ .)
penguin_wf
```

As it is seen, there is no model set yet.

### Training Logistic Regression

Let's add logistic regression model. I can fit it directly to the training sample.

```{r}
penguin_wf %>% 
   add_model(glm_spec) %>% 
   fit(data = penguins_train)
```

I get the coefficients and other detains for my model which is great.

However, as I said before I can't be absolutely sure of my model right away because of small sample. So, I will use bootstrapped samples that I created earlier. `verbose = T` will show me all steps.

```{r}
glm_rs = penguin_wf %>% 
   add_model(glm_spec) %>%
   fit_resamples(resamples = penguins_boot, 
                 control = control_resamples(save_pred = TRUE, verbose = F))

glm_rs
```

One bootstrapped sample had sampling issues (they training labels were unbalanced). To solve this, I could have specified `strata = sex` in `bootstraps()`. In this case it is acceptable because 24 of them worked well.

### Training Random Forest

The process is almost the same as that for logistic regression.

```{r}
rf_rs = penguin_wf %>% 
   add_model(rf_spec) %>%
   fit_resamples(resamples = penguins_boot, 
                 control = control_resamples(save_pred = TRUE, verbose = F))

rf_rs
```

Notice that I did not get the same warning in random forest model. Why? Because random forest is not probabilistic in nature. Tree based models do not necessitate presence of balanced samples. They will simply give biased results and it is up to the researcher to investigate the flaws. That's why they are little tricky.

# Evaluation

How well do they compare against each other? The metrics to compare can be obtained using `collect_metrics()`.

## Logistic Regression Metrics

```{r}
collect_metrics(glm_rs)
```

## Random Forest Metrics

```{r}
collect_metrics(rf_rs)
```

Logistic regression performs slightly better in both metrics: accuracy and AUC. Even if they were nearly equal and I had to choose, I would choose linear model. It is faster to implement, scalable and most importantly explainable.

## Predictions

The predictions can be found using `collect_predictions()` function.

```{r}
glm_rs %>% 
  collect_predictions()
```

The two important columns in this are `.pred_female` and `.pred_male`. This is the probability that they belong to a particular class. `.pred_class` gives the class that our model predicts a penguin to be in; `sex` is their true class.

## Confusion Matrix

`conf_mat_resampled()` with no arguments gives confusion matrix in tidy format.

```{r}
glm_rs %>% 
  conf_mat_resampled()
```

Let's see it in conventional format.

```{r}
glm_rs %>% 
  collect_predictions() %>% 
  conf_mat(sex, .pred_class)
```

The model looks pretty good.

## ROC Curve

The ROC curve can be produced by `roc_curve()` function. `autoplot()` uses default settings.

```{r}
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female) %>% 
   autoplot()
```

How does `autoplot()` work?

```{r}
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female)
```

Let's beautify our ROC curve.

```{r}
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female) %>% 
   ggplot(aes(1 - specificity, sensitivity, col = id)) +
   geom_abline(lty = 2, colour = "grey80", size = 1.5) +
   geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
   coord_equal()
```

I'm using `geom_path` instead of `geom_line` because I want to see discrete jumps. `geom_line` would give me a continious plot as it connects the points in the order of the variable on the x-axis. Another option is `geom_step` which only highlights changes --- when a variable steps to take another value.

```{r}
# Using geom_line()
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female) %>% 
   ggplot(aes(1 - specificity, sensitivity, col = id)) +
   geom_abline(lty = 2, colour = "grey80", size = 1.5) +
   geom_line(show.legend = FALSE, alpha = 0.6, size = 1.2) +
   coord_equal()

# Using geom_step()
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female) %>% 
   ggplot(aes(1 - specificity, sensitivity, col = id)) +
   geom_abline(lty = 2, colour = "grey80", size = 1.5) +
   geom_step(show.legend = FALSE, alpha = 0.6, size = 1.2) +
   coord_equal()
```

`geom_abline()` can take two arguments: intercept and slope. If you provide none of these, it plots $y = x$. So, the best plot is from `geom_path()`.

```{r}
glm_rs %>% 
   collect_predictions() %>% 
   group_by(id) %>% # -- to get 25 ROC curves, for each bootstrapped sample
   roc_curve(sex, .pred_female) %>% 
   ggplot(aes(1 - specificity, sensitivity, col = id)) +
   geom_abline(lty = 2, colour = "grey80", size = 1.5) +
   geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
   coord_equal()
```

Okay, enough on `ggplot2`. From the above ROC curves, we can deduce the model is doing great for the training samples.

## Testing Samples

The above metrics used only the test data. We need to check our model's performance on test dataset too.

Let's fit the model using `last_fit()`. `last_fit()` fits the final best model to the training set and evaluates the test set.

```{r}
penguins_final = penguin_wf %>% 
  add_model(glm_spec) %>% 
  last_fit(penguins_split)

penguins_final
```

Let's check how good our final model is.

```{r}
penguins_final %>% 
  collect_metrics()

penguins_final %>% 
  collect_predictions()
```

Our model is 90.5% accurate and has AUC of 0.966. These are very high accuracies.

### Confusion Matrix for Test Set

```{r}
penguins_final %>% 
  collect_predictions() %>% 
  conf_mat(sex, .pred_class)
```

This does pretty good to be honest.

### Final Workflow

Let's see the final model.

```{r}
penguins_final$.workflow[[1]]
```

Can we tidy it up? (We need `[[1]]` to get the element out as `.workflow` is a list.

```{r}
penguins_final$.workflow[[1]] %>% 
  tidy()
```

The coefficients can be exponentiated to find the odds ratio.

```{r}
penguins_final$.workflow[[1]] %>% 
   tidy(exponentiate = TRUE) %>% 
   arrange(estimate)
```

The coefficient of 3.75 means that for every one mm increase in bill depth, the odds of being male increases by almost eight times. So, bill depth is very important.

Flipper value are not very important. Remember that previously we explored the relationship between sex and flipper length. If flipper length is not important (high p-value), let's see how the graph would look like with bill depth which is apparently very important.

```{r}
penguins %>% 
   filter(!is.na(sex)) %>% 
   ggplot(aes(bill_depth_mm, bill_length_mm, colour = sex, size = body_mass_g)) +
   geom_point(alpha = 0.6) +
   facet_wrap(~species)
```

# Conclusion

In this creating this short tutorial, I learnt how to classify data using tidymodels workflow with logistic regression and random forest. An important lesson was that logistic regression can outperform complicated trees like random forest too.

------------------------------------------------------------------------

If you are interested in data science and R, check out my [free weekly newsletter](https://www.getrevue.co/profile/harshbutjust) **Next**.

> ### Next --- Today I Learnt About R
>
> A short and sweet curated collection of R-related works. Five stories. Four packages. Three jargons. Two tweets. One Meme.

You can read the past editions and subscribe to it [here](https://www.getrevue.co/profile/harshbutjust).
